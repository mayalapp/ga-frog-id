{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo: \n",
    "- [ ] Check if gopher frog files are grouped - do they happen a bunch of files in a row? Bunch of days in a row? \n",
    "- [ ] Clean up EDA\n",
    "- [ ] Clean parameter selection for use in methods \n",
    "- [ ] Still losing some files when merging - why??\n",
    "\n",
    "#*# - indicates locations where you may want to edit (e.g. file paths, parameter values, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages and define settings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress warnings\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "#import packages\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "from plotnine import *\n",
    "import os\n",
    "\n",
    "pd.options.display.max_colwidth = 200\n",
    "\n",
    "\n",
    "#local imports from opensoundscape\n",
    "from opensoundscape.audio import Audio\n",
    "from opensoundscape.spectrogram import Spectrogram\n",
    "from opensoundscape.ribbit import ribbit\n",
    "\n",
    "# create big visuals\n",
    "plt.rcParams['figure.figsize']=[15,8]\n",
    "pd.set_option('display.precision', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine multiple ribbit score csv files into one csv file. Useful when you break the data into several groups to \n",
    "# run the model simultaneously on those groups and then want to recombine the data into one dataframe\n",
    "# Inputs: \n",
    "# folder - folder with all of the ribbit score csv files that should be combined. \n",
    "#    csv files must have the exact same columns \n",
    "#    e.g. rs_folder = \"./ribbit_scores_flshe/\"\n",
    "# delete_files - if True, delete the old csv files and only keep the combined file \n",
    "# Results: \n",
    "# creates new csv file with all of the ribbit scores in one csv file. deletes all of the individual files\n",
    "# returns the concatonated ribbit score dataframe \n",
    "\n",
    "def combine_csvs(folder_path, new_csv_name = \"combined_data.csv\", delete_files = False): \n",
    "    files = glob(folder_path+ \"*.csv\") #create list of all csv files in rs_folder \n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    for fp in files: \n",
    "        if(os.path.exists(fp) and os.path.isfile(fp)): # check whether file exists or not\n",
    "            temp_df = pd.read_csv(fp, index_col = 0) # read in ribbit scores\n",
    "            df = pd.concat([df, temp_df]) # add it to the new dataframe \n",
    "            \n",
    "            print(fp + \" concatenated\")\n",
    "            if(delete_files): # delete old files\n",
    "                os.remove(fp)\n",
    "                print(\"file deleted\")\n",
    "        else:\n",
    "            print(fp + \" not found\")\n",
    "\n",
    "    df.to_csv(folder_path + new_csv_name) # save dataframe to csv file \n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# calculate prescision/recall for a model \n",
    "# Inputs: \n",
    "# df - dataframe with ribbit scores and \n",
    "# cutoffs - list of cutoff values \n",
    "# score_col - name of column with RIBBIT score \n",
    "# Outputs: \n",
    "# accuracy - dataframe with precision and recall for the ribbit scores \n",
    "def calc_accuracy(df, cutoffs, score_col = \"score\"):    \n",
    "    accuracy = pd.DataFrame()\n",
    "    accuracy.index = list(cutoffs)\n",
    "    accuracy['precision'] = ''\n",
    "    accuracy['recall'] = ''\n",
    "\n",
    "    for c in cutoffs: \n",
    "        accuracy['precision'][c] = len(df[(df['Lcapito']==1) & (df[score_col]>c)])/len(df[df[score_col]>c])*100\n",
    "        accuracy['recall'][c] = len(df[(df['Lcapito'] == 1) & (df[score_col]>c)])/len(df[(df['Lcapito'] == 1)])*100\n",
    "\n",
    "    accuracy = accuracy.astype(int)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# get list of audio files with top ribbit scores\n",
    "#\n",
    "# input: \n",
    "# df - data frame with ribbit scores \n",
    "# n - number of files per group (e.g. n = 5 gets top 5 ribbit scores per group)\n",
    "# min_score - minimum ribbit score needed for file to be included \n",
    "# (e.g. if you want all files above a ribbit score of 50, min_score = 50 and n = 999999999999)\n",
    "# group_col - the name of the column with the labels grouping our files \n",
    "#            (e.g. \"pond\" for sandhills or \"Site\" for ichaway wetlands)\n",
    "#             if group_col = \"\", just takes overall top scores \n",
    "# groups - list of labels of the different categories we want to split the data into - e.g. \"range(398, 403)\"\" for sandhills ponds or \"ich_wetlands\" for ichaway wetlands\n",
    "# t_unit - unit for how often we want the top scores (options: D, W, M, Y - day, week, month, year)\n",
    "#\n",
    "# out: \n",
    "# dataframe with top `n` files with ribbit score over `min_score` for each `groups` for every `t_unit` \n",
    "def get_top_rs(df, n = 5, min_score = 0.0, group_col = 'no_groups', groups = range(398, 403), t_unit = \"Y\", score_col = \"score\", time_stamp_col = \"time_stamp\"):\n",
    "    \n",
    "    rs_df = df[df[score_col] > min_score] # only keep files with scores above the minimum allowed ribbit score\n",
    "    \n",
    "    if group_col == 'no_groups': # if we don't want to group the data - just want overall top scores\n",
    "        rs_df['no_groups'] = \"0\"\n",
    "        groups = [\"0\"]\n",
    "\n",
    "    rs_df = rs_df[rs_df[group_col].isin(groups)]\n",
    "    rs_df['date_group'] = rs_df['date'].dt.to_period(t_unit)\n",
    "\n",
    "\n",
    "    out_df = rs_df.sort_values(by=score_col,ascending=False).groupby(by = [group_col, 'date_group']).head(n).sort_values(by = [group_col, 'year', 'date_group'])\n",
    "\n",
    "\n",
    "        # organize the df\n",
    "    out_df['file_path'] = out_df['file_path'].str.slice(61) # simplify file path to not have beginning crap\n",
    "    \n",
    "    first_cols = ['Lcapito',group_col,'date_group', score_col, time_stamp_col, 'file_path']\n",
    "    last_cols = [col for col in out_df.columns if (col not in first_cols)]\n",
    "    out_df = out_df[first_cols+last_cols]\n",
    "    out_df = out_df.drop(labels = [\"Unnamed: 0\"], axis = 1)\n",
    "    \n",
    "    if 'no_groups' in out_df.columns:\n",
    "        out_df = out_df.drop(labels = ['no_groups'], axis = 1)\n",
    "    #out_df = out_df.reindex(columns = ['Lcapito', group_col, 'date_group', score_col, time_stamp_col, 'file_path']) #order columns \n",
    "\n",
    "    \n",
    "    return out_df\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FLSHE data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine multiple ribbit score csv files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only need to run if you need to combine ribbit scores from multiple csv files  \n",
    "# Useful if you broke up a model run into section to run it faster \n",
    "# WARNING: if delete_files = True this will delete individual files after combining them\n",
    "# keep next 2 lines commented out unless running this chunk to avoid deleting files unintentionally \n",
    "\n",
    "# folder_path = \"./ribbit_scores_flshe_20221206/\" #*# path to folder containing the csv files you want to combine \n",
    "# rs_flshe = combine_csvs(folder_path, new_csv_name = \"ribbit_scores_combined.csv\", delete_files = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up file and folder paths for data import and cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file path to csv file with ribbit scores \n",
    "ribbit_scores_fp = \"./ribbit_scores_flshe_20221206/ribbit_scores_combined.csv\" #*#\n",
    "\n",
    "# file path to csv file with manually verified data \n",
    "verified_data_fp = \"../manually_verified_data/FLSHE_pond400.csv\" #*#\n",
    "\n",
    "# path to folder containing audio files \n",
    "audio_files_fp = '/Volumes/Expansion/Frog Call Project/Calling Data/FLSHE/' #*#\n",
    "# Note: if the folders within this folder are structured differently, you may need to edit the full file paths in the \n",
    "#       data cleaning section below (inicated with #*#)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and clean csv file with ribbit scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ribbit scores based on ribbit_scores_fp\n",
    "rs_flshe = pd.read_csv(ribbit_scores_fp, index_col = 0)\n",
    "\n",
    "# extract date from file path \n",
    "rs_flshe['date'] = pd.to_datetime(rs_flshe.index.str[-19:-4], format='%Y%m%d_%H%M%S', errors='coerce') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and clean manually verified data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import manually verified data \n",
    "verified_flshe = pd.read_csv(verified_data_fp)[[\"File name\", \"Pond #\", \"L. capito\", \"gopher call time\", \"Date\"]] # keeps only listed columns \n",
    "\n",
    "# rename columns for convenience\n",
    "verified_flshe = verified_flshe.rename(columns = {\"File name\":\"file_name\", \"Pond #\":\"logger\", \"L. capito\":\"Lcapito\", \"gopher call time\":\"call_time\", \"Date\":\"date\"})\n",
    "\n",
    "# make Lcapito categorical\n",
    "verified_flshe.Lcapito = verified_flshe.Lcapito.astype(\"category\")\n",
    "\n",
    "# create year column based on date string\n",
    "verified_flshe['year'] = verified_flshe.date.str[0:4]\n",
    "verified_flshe.astype({\"year\":\"int\"})\n",
    "\n",
    "# add .wav to file name if it is not included with the file name \n",
    "for i in verified_flshe.index:\n",
    "    if verified_flshe[\"file_name\"][i][-4:] != \".wav\": \n",
    "        verified_flshe[\"file_name\"][i] = verified_flshe[\"file_name\"][i] + \".wav\"\n",
    "    \n",
    "#*# create full file path from file names, year, and logger numbers #*# \n",
    "verified_flshe['file_path'] = audio_files_fp + 'FLSHE_' + \\\n",
    "    verified_flshe['year'].astype('string') + \\\n",
    "    '/FLSHE_' + verified_flshe['year'].astype('string') + '_' + verified_flshe['logger'].astype('string') + '/' + \\\n",
    "    verified_flshe['file_name'] #*#\n",
    "\n",
    "# set file path as index \n",
    "verified_flshe = verified_flshe.set_index('file_path')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge ribbit scores to manually verified data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with ribbit scores data file \n",
    "verified_flshe = verified_flshe.drop(columns = [\"year\", \"date\", \"logger\"]).merge(rs_flshe, left_index = True, right_index = True)\n",
    "verified_flshe = verified_flshe.dropna(subset=['Lcapito']) # drop any rows with \"NaN\" for Lcapito - if left empty, etc. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ichaway data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine multiple ribbit score csv files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only need to run if you need to combine ribbit scores from multiple csv files  \n",
    "# Useful if you broke up a model run into section to run it faster \n",
    "# WARNING: if delete_files = True this will delete individual files after combining them\n",
    "# keep next 2 lines commented out unless running this chunk to avoid deleting files unintentionally \n",
    "\n",
    "# folder_path = \"./ribbit_scores_ich_20221206/\" #*# path to folder containing the csv files you want to combine \n",
    "# rs_ich = combine_csvs(folder_path, new_csv_name = \"ribbit_scores_combined.csv\", delete_files = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine multiple csv files with manually verified data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this once! if you need to combine manually verified data files \n",
    "# Data must have the following columns: \"Site\", \"Logger\", \"Sample Date\", \"Species\", \"NAAMP\", \"File ID\"\n",
    "# folder_path = \"../manually_verified_data/ichaway_verified_data/\"\n",
    "# raw_ich = combine_csvs(folder_path, new_csv_name = \"ichaway_verified_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up file and folder paths for data import and cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file path to csv file with ribbit scores \n",
    "ribbit_scores_fp = \"./ribbit_scores_ich_20221206/ribbit_scores_combined.csv\" #*#\n",
    "\n",
    "# file path to csv file with manually verified data \n",
    "verified_data_fp = \"../manually_verified_data/ichaway_verified_data/ichaway_verified_data.csv\" #*#\n",
    "\n",
    "# path to folder containing audio files \n",
    "audio_files_fp = '/Volumes/Expansion/Frog Call Project/Calling Data/ichaway/' #*#\n",
    "# Note: if the folders within this folder are structured differently, you may need to edit the full file paths in the \n",
    "#       data cleaning section below (inicated with #*#)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and clean csv files with ribbit scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ribbit scores based on ribbit_scores_fp\n",
    "rs_ich = pd.read_csv(ribbit_scores_fp, index_col = 0)\n",
    "\n",
    "# extract date from file path \n",
    "rs_ich['date'] = pd.to_datetime(rs_ich.index.str[-19:-4], format='%Y%m%d_%H%M%S', errors='coerce') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and clean manually verfied files data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import manually verified data\n",
    "raw_ich = pd.read_csv(verified_data_fp)[[\"Site\", \"Logger\", \"Sample Date\", \"Species\", \"NAAMP\", \"File ID\", \"Start Date\"]]\n",
    "\n",
    "# rename columns for convenience\n",
    "raw_ich = raw_ich.rename(columns = {\"Site\":\"site\", \"Logger\":\"logger\", \"Sample Date\":\"date\", \"Species\":\"species\", \"File ID\":\"file_name\", \"Start Date\":\"folder_date\"})\n",
    "\n",
    "# create year column based on date string\n",
    "raw_ich['year'] = raw_ich.date.astype(str).str[-4:]\n",
    "raw_ich.astype({\"year\":\"int\"})\n",
    "\n",
    "\n",
    "# create full file path from file names andd logger numbers \n",
    "raw_ich['folder_date'] = pd.to_datetime(raw_ich['folder_date'], format='%m/%d/%Y').dt.strftime('%-m-%-d-%y')\n",
    "raw_ich['file_path'] = audio_files_fp + 'ichaway_' + raw_ich['year'].astype('string') + '/' + raw_ich['logger'].astype('string') + 'a/' + raw_ich['folder_date'] + '/' + raw_ich['file_name'] + '.wav' #*#\n",
    "# set file path as index \n",
    "raw_ich = raw_ich.set_index('file_path')\n",
    "\n",
    "# identify which rows are Lcapito observations \n",
    "raw_ich['Lcapito'] = raw_ich['species'] == 'LICAP'\n",
    "raw_ich['Lcapito'] = raw_ich['Lcapito'].astype('category')\n",
    "\n",
    "# create \"verified_ich\" dataframe with one row per file with a column (Lcapito) with 1 if the file has a Lcapito and 0 if it does not\n",
    "verified_ich = raw_ich.sort_values([\"file_path\", \"Lcapito\"], ascending = False).groupby('file_path').head(1) \n",
    "\n",
    "# these files were labeled incorrectly in the ichaway data - there are gopher frogs in them \n",
    "# logger 5a: \n",
    "#20150205_194700\n",
    "#20150205_204700\n",
    "#20150205_214700\n",
    "\n",
    "# fix these mistakes\n",
    "temp = audio_files_fp + 'ichaway_2015/5a/2-2-15/20150205_'\n",
    "incorrect_files =  [temp + '194700.wav', temp + '204700.wav', temp + '214700.wav']\n",
    "verified_ich.loc[verified_ich.index.isin(incorrect_files),'Lcapito'] = True\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge ribbit scores and verified files  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge option 1 - use this if option 2 is causing issues \n",
    "\n",
    "# merge with ribbit scores based on file path\n",
    "# this drops some files where the file path minutes don't match between the rs_ich and verified_ich\n",
    "#verified_ich = verified_ich.drop(columns = [\"year\", \"date\", \"logger\"]).merge(rs_ich, left_index = True, right_index = True)\n",
    "#verified_ich = verified_ich.dropna(subset=['Lcapito']) # drop any rows with \"NaN\" for Lcapito - if data was entered incorrectly, empty, etc. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge option 2\n",
    "\n",
    "# merge with ribbit scores based on hour of file path (ignore minutes - these sometimes don't match for some reason)\n",
    "# still drops some files but not as many \n",
    "# warning: potential to match incorrect files (e.g. if one file is labeled 10:01 and another 10:58)\n",
    "rs_ich[\"fp_shortened\"] = rs_ich.index.str[:-8]\n",
    "verified_ich[\"fp_shortened\"] = verified_ich.index.str[:-8]\n",
    "verified_ich = verified_ich.drop(columns = [\"year\", \"date\", \"logger\"]).merge(rs_ich, left_on = \"fp_shortened\", right_on = \"fp_shortened\")\n",
    "verified_ich = verified_ich.dropna(subset=['Lcapito']).drop(columns = [\"fp_shortened\"]) # drop any rows with \"NaN\" for Lcapito - if data was entered incorrectly, empty, etc. \n",
    "\n",
    "### TODO:  still losing some files after merging - why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OpenSoundscape060",
   "language": "python",
   "name": "opensoundscape060"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
