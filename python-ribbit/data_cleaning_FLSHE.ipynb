{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the file setup_functions.ipynb to define setting, import packages, and define functions \n",
    "%run ./setup_functions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo: \n",
    "- [ ] Check if gopher frog files are grouped - do they happen a bunch of files in a row? Bunch of days in a row? \n",
    "- [ ] Clean parameter selection for use in methods \n",
    "\n",
    "#*# - indicates locations where you may want to edit (e.g. file paths, parameter values, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine multiple ribbit score csv files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only need to run if you need to combine ribbit scores from multiple csv files  \n",
    "# Useful if you broke up a model run into section to run it faster \n",
    "# WARNING: if delete_files = True this will delete individual files after combining them\n",
    "# keep next 2 lines commented out unless running this chunk to avoid deleting files unintentionally \n",
    "\n",
    "# folder_path = \"./ribbit_scores_flshe_20221206/\" #*# path to folder containing the csv files you want to combine \n",
    "# rs_flshe = combine_csvs(folder_path, new_csv_name = \"ribbit_scores_combined.csv\", delete_files = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define file and folder paths for data import and cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file path to csv file with ribbit scores \n",
    "ribbit_scores_fp = \"./ribbit_scores_flshe_20221206/ribbit_scores_combined.csv\" #*#\n",
    "\n",
    "# file path to csv file with manually verified data \n",
    "verified_data_fp = \"../manually_verified_data/FLSHE_pond400.csv\" #*#\n",
    "\n",
    "# path to folder containing audio files \n",
    "audio_files_fp = '/Volumes/Expansion/Frog Call Project/Calling Data/FLSHE/' #*#\n",
    "# Note: if the folders within this folder are structured differently, you may need to edit the full file paths in the \n",
    "#       data cleaning section below (inicated with #*#)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and clean RIBBIT score data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ribbit scores based on ribbit_scores_fp\n",
    "rs_flshe = pd.read_csv(ribbit_scores_fp, index_col = 0)\n",
    "\n",
    "# extract date from file path \n",
    "rs_flshe['date'] = pd.to_datetime(rs_flshe.index.str[-19:-4], format='%Y%m%d_%H%M%S', errors='coerce') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and clean manually verified data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import manually verified data \n",
    "verified_flshe = pd.read_csv(verified_data_fp)[[\"File name\", \"Pond #\", \"L. capito\", \"gopher call time\", \"Date\"]] # keeps only listed columns \n",
    "\n",
    "# rename columns for convenience\n",
    "verified_flshe = verified_flshe.rename(columns = {\"File name\":\"file_name\", \"Pond #\":\"logger\", \"L. capito\":\"Lcapito\", \"gopher call time\":\"call_time\", \"Date\":\"date\"})\n",
    "\n",
    "# make Lcapito categorical\n",
    "verified_flshe.Lcapito = verified_flshe.Lcapito.astype(\"category\")\n",
    "\n",
    "# create year column based on date string\n",
    "verified_flshe['year'] = verified_flshe.date.str[0:4]\n",
    "verified_flshe.astype({\"year\":\"int\"})\n",
    "\n",
    "# add .wav to file name if it is not included with the file name \n",
    "for i in verified_flshe.index:\n",
    "    if verified_flshe[\"file_name\"][i][-4:] != \".wav\": \n",
    "        verified_flshe[\"file_name\"][i] = verified_flshe[\"file_name\"][i] + \".wav\"\n",
    "    \n",
    "#*# create full file path from file names, year, and logger numbers #*# \n",
    "verified_flshe['file_path'] = audio_files_fp + 'FLSHE_' + \\\n",
    "    verified_flshe['year'].astype('string') + \\\n",
    "    '/FLSHE_' + verified_flshe['year'].astype('string') + '_' + verified_flshe['logger'].astype('string') + '/' + \\\n",
    "    verified_flshe['file_name'] #*#\n",
    "\n",
    "# set file path as index \n",
    "verified_flshe = verified_flshe.set_index('file_path')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge ribbit scores to manually verified data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with ribbit scores data file \n",
    "verified_flshe = verified_flshe.drop(columns = [\"year\", \"date\", \"logger\"]).merge(rs_flshe, left_index = True, right_index = True)\n",
    "verified_flshe = verified_flshe.dropna(subset=['Lcapito']) # drop any rows with \"NaN\" for Lcapito - if left empty, etc. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OpenSoundscape060",
   "language": "python",
   "name": "opensoundscape060"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
